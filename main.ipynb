{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b9c1b9",
   "metadata": {},
   "source": [
    "# GCRL2000 - NLP Functions\n",
    "- This notebook contains various functions for processing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e09bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import TweetTokenizer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "default_stopwords = stopwords.words('english')\n",
    "\n",
    "default_stopwords.extend(\n",
    "    list(string.punctuation) + ['would', 'could', 'get',\n",
    "                                'want', 'he', 'twitter']\n",
    ")\n",
    "\n",
    "default_tokeniser = TweetTokenizer()\n",
    "default_lemmatiser = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb2595",
   "metadata": {},
   "source": [
    "- stopwords contain words to remove from the tweets, e.g., \"the\"\n",
    "- wordnet is used by the WordNetLemmatiser\n",
    "- averaged_perceptron_tagger is used by pos_tagger(), it tags a word into nouns, verbs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e959dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_nltk_downloads() -> None:\n",
    "    \"\"\"\n",
    "    Download the necessary resources for nltk, such as stopwords\n",
    "    \"\"\"\n",
    "    resources = ['corpora/stopwords', 'corpora/wordnet',\n",
    "                 'taggers/averaged_perceptron_tagger']\n",
    "    for resource in resources:\n",
    "        # Find .zip file instead since nltk have problem unzipping files\n",
    "        try:\n",
    "            nltk.find(f'{resource}.zip')\n",
    "        except LookupError:\n",
    "            nltk.download(resource.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53c32a",
   "metadata": {},
   "source": [
    "- Regex explanation:\n",
    "    - `r\"(@[A-Za-z0-9_]+)\"` will match any string, starting with \"@\", containing letters, numbers, or underscore\n",
    "    - `r\"^http.+?|(\\w+:\\/\\S+)\"` match strings starting with \"http\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210756ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_twitter_csv(data_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load in a csv file produce by Twitter scraper, return cleaned DataFrame\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        data_file,\n",
    "        index_col=0,\n",
    "        usecols=['conversation_id', 'created_at', 'user_id', 'tweet',\n",
    "                 'language']\n",
    "    )\n",
    "    # Only consider English tweets, ignore neutral language\n",
    "    df.query('language == \"en\"', inplace=True)\n",
    "    df.drop(columns=['language'], inplace=True)\n",
    "    df.dropna(subset=['tweet'], inplace=True)\n",
    "\n",
    "    # Clean tweet texts\n",
    "    df['tweet'] = \\\n",
    "        df['tweet'].apply(\n",
    "            lambda txt: re.sub(\n",
    "                r\"(@[A-Za-z0-9_]+)|\"        # Match mentions\n",
    "                r\"^http.+?|(\\w+:\\/\\S+)\",    # Match urls\n",
    "                '',\n",
    "                txt\n",
    "            ).lower()\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d8d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and turn word into lemmatised form\n",
    "def furnish(\n",
    "        text: str, \n",
    "        tokeniser=default_tokeniser, \n",
    "        lemmatiser=default_lemmatiser, \n",
    "        stop_words: list = default_stopwords\n",
    "):\n",
    "    final_text = []\n",
    "    for word, tag in pos_tag(tokeniser.tokenize(text)):\n",
    "        # Tag word as verb, nouns, etc, improves lemmatiser accuracy\n",
    "        tag = tag.lower()[0]\n",
    "        tag = tag if tag in ['a', 'r', 'n', 'v'] else None\n",
    "        if tag:\n",
    "            word = lemmatiser.lemmatize(word, tag)\n",
    "        else:\n",
    "            word = lemmatiser.lemmatize(word)\n",
    "        if word not in stop_words:\n",
    "            final_text.append(word)\n",
    "    return ' '.join(final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cf94b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_n_grams(\n",
    "        df: pd.DataFrame, new_col_name=\"tweet_n_gram\", min_len=1, max_len=3\n",
    "):\n",
    "    df[new_col_name] = df['tweet'].apply(lambda x: list(\n",
    "        nltk.everygrams(x.split(' '), min_len=min_len, max_len=max_len))\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5893fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_words(\n",
    "        df: pd.DataFrame,\n",
    "        col: str,\n",
    "        vectoriser,\n",
    "        decomposer,\n",
    "):\n",
    "    term_matrix = vectoriser.fit_transform(df[col].values.astype('U'))\n",
    "\n",
    "    decomposer.fit(term_matrix)\n",
    "    return vectoriser, decomposer, term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99046d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_words(decomposer, vectoriser, n_words):\n",
    "    for i, topic in enumerate(decomposer.components_):\n",
    "        print(f'Top {n_words} words for topic #{i}:')\n",
    "        print([vectoriser.get_feature_names_out()[i]\n",
    "               for i in topic.argsort()[-n_words:]])\n",
    "        print('\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6a92422",
   "metadata": {},
   "source": [
    "TODO: Bag-of-words \n",
    "LIWC\n",
    "\n",
    "TUI\n",
    "jupyter\n",
    "OOP - refactor\n",
    "send kaggle email\n",
    "\n",
    "\n",
    "look at keywords and raed tweet - uncommon words\n",
    "- freq of tweets containing keyword\n",
    "- sentiment as dimension in clustering\n",
    "- conduct on subset of tweet\n",
    "- hedonometre - compare with VADER\n",
    "    - gorup of tweets\n",
    "- labMT package\n",
    "\n",
    "\n",
    "- can ChatGTP do narrative analysis?\n",
    "    - give it the data? can it find narrative?\n",
    "    - look at Kaggle - issue, findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f867b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "216369ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import init_nltk_downloads, load_twitter_csv, furnish, get_topic_words, print_topic_words\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a06ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../Dataset/twitter.csv\"\n",
    "\n",
    "init_nltk_downloads()\n",
    "df = load_twitter_csv(data_file)\n",
    "\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(furnish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f236de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec, nmf, tfidf_term_matrix = get_topic_words(\n",
    "    df, 'tweet',\n",
    "    TfidfVectorizer(max_df=0.95, min_df=5),\n",
    "    NMF(n_components=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294a9d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec, lda, count_term_matrix = get_topic_words(\n",
    "    df, 'tweet', CountVectorizer(max_df=0.95, min_df=4),\n",
    "    LatentDirichletAllocation(n_components=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "779b4244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 words for topic #0:\n",
      "['thing', 'settle', 'out', 'weird', 'lawsuit', 'texted', '44', 'lawyer', 'pull', 'report', 'billion', 'ceo', 'deal', 'elon', 'musk']\n",
      "\n",
      "\n",
      "Top 15 words for topic #1:\n",
      "['year', 'next', 'wait', 'push', 'engadget', 'battle', 'elon', 'musk', 'trial', '2023', 'february', 'court', 'start', 'ask', 'delay']\n",
      "\n",
      "\n",
      "Top 15 words for topic #2:\n",
      "['right', 'fake', 'see', 'tweet', 'one', 'people', 'make', 'say', 'think', 'know', 'go', 'like', 'account', 'bot', 'buy']\n",
      "\n",
      "\n",
      "Top 15 words for topic #3:\n",
      "['speed', 'push', 'report', 'file', 'motion', 'twitter', 'elon', 'track', 'fast', 'musk', 'block', 'seek', 'expedited', 'request', 'trial']\n",
      "\n",
      "\n",
      "Top 15 words for topic #0:\n",
      "['make', 'may', 'dollar', 'contract', 'give', 'sue', '44', 'go', 'deal', 'force', 'court', 'buy', 'billion', 'musk', 'elon']\n",
      "\n",
      "\n",
      "Top 15 words for topic #1:\n",
      "['one', 'right', 'stock', 'people', 'deal', 'make', 'say', 'company', 'think', 'back', 'bot', 'go', 'buy', 'musk', 'elon']\n",
      "\n",
      "\n",
      "Top 15 words for topic #2:\n",
      "['see', 'fake', 'think', 'one', 'number', 'user', 'say', 'people', 'day', 'use', 'tweet', 'know', 'like', 'account', 'bot']\n",
      "\n",
      "\n",
      "Top 15 words for topic #3:\n",
      "['via', 'pull', 'elonmusk', 'lawyer', 'ask', 'lawsuit', 'ceo', 'report', 'request', 'court', 'deal', 'twitter', 'trial', 'elon', 'musk']\n",
      "\n",
      "\n",
      ":D\n"
     ]
    }
   ],
   "source": [
    "print_topic_words(nmf, tfidf_vec, 15)\n",
    "print_topic_words(lda, count_vec, 15)\n",
    "\n",
    "\n",
    "print(\":D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e64002-b44a-43f9-8a83-7feb4b407ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcrl2000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "061547610e8fceca887a10faa2400f70de22abf81f0fbff289301edcce13ef23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
