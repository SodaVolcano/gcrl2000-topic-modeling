{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b948655-243b-4b45-86e7-227388bb6bb7",
   "metadata": {},
   "source": [
    "# `sklearn` Explanations\n",
    "- This notebook contains explanation to the various tools available in `sklearn` and some theory behind how they work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8c234-4073-4167-858a-5232bda0b1a1",
   "metadata": {},
   "source": [
    "## Vectorisers\n",
    "- **Vectoriser**\n",
    "    - Converts an `iterable` containing words into a matrix of real numbers - the **term matrix**, which can be used for further analysis such as clustering\n",
    "    - e.g., `TfidfVectoriser` and `CountVectoriser`\n",
    "    - **NOTE**: after producing a term matrix, the vectoriser will store the feature names (i.e. the terms it analysed). Use `get_feature_names_out()` to see the features\n",
    "- **Term matrix**\n",
    "    - Each column represent a unique feature (term) that appeared in the corpus\n",
    "    - Each row represent each document in a corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd290f-e137-41fa-8342-d3063118deb2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44fbc69-c3c2-437f-aba9-25de3a0fb0df",
   "metadata": {},
   "source": [
    "### TF-IDF - Term frequency-Inverse Document Frequency\n",
    "- Vectoriser that take in words and produce a matrix containing weighting for each word\n",
    "- the weight reflect how *important* a word is to a *document* in a collection\n",
    "- For each word, the weight is the product of these two below:\n",
    "    - **Term frequency**: How many times the term occurs, calculated for *each* document\n",
    "    $$\\text{tf}(t, d) = \\frac{\\text{raw count of term }t}{\\text{sum of frequency for all terms in document }d}$$\n",
    "    - **Inverse document frequency**: number of documents divided by number of documents the word occured in, scaled logarithmically\n",
    "    $$\\text{idf}(t, D) = \\log \\frac{\\text{total number of documents in collection }D}{\\text{number of documents that term }t \\text{ occured in}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80333c29-56cf-4cd0-9a39-081aad37d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['the mitochondria is the power house of the cell', 'the factory of the sky is old']\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb1e50ad-036c-499c-9dd9-d6bc35396093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cell' 'factory' 'house' 'is' 'mitochondria' 'of' 'old' 'power' 'sky'\n",
      " 'the']\n",
      "[[0.32327633 0.         0.32327633 0.23001377 0.32327633 0.23001377\n",
      "  0.         0.32327633 0.         0.6900413 ]\n",
      " [0.         0.40697968 0.         0.2895694  0.         0.2895694\n",
      "  0.40697968 0.         0.40697968 0.57913879]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vect.get_feature_names_out())\n",
    "\n",
    "# Words appearing once such as \"cell\", \"mitochondria\", \"factory\" has higher\n",
    "# weight than \"is\", \"of\"\n",
    "# Note that \"the\" has high weight because the corpus is small\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f135d3-32fb-4447-a4b3-435141c29458",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583459ba-a264-41d3-9f13-4191cc965ae4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Count Vectoriser (Bag-of-Words)\n",
    "- Vectoriser that count up and return frequency of each term in a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b759abc-1d23-49c7-91b2-8000869bf5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "corpus = ['Cats and dogs are not allowed', 'Cats and dogs are antagonistic dogs']\n",
    "count_vect = CountVectorizer()\n",
    "bag_of_words = count_vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b4c956-260a-4359-ad71-617dc4be00e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allowed' 'and' 'antagonistic' 'are' 'cats' 'dogs' 'not']\n",
      "[[1 1 0 1 1 1 1]\n",
      " [0 1 1 1 1 2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.get_feature_names_out())\n",
    "\n",
    "# There are 2 arrays (since we have 2 documents in our corpus)\n",
    "# Each array have 7 items as we have 7 unique words throughout all documents\n",
    "print(bag_of_words.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bf53b2a",
   "metadata": {},
   "source": [
    "## Clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "279143da",
   "metadata": {},
   "source": [
    "#### Optimal Cluster\n",
    "- `KMeans` privides the `inertia_` attribute - sum of square distances of samples to their cluster's centre, i.e. the **SSE**\n",
    "    - SSE is a measure for how fitted the model is to the data, low SSE means the model is very fitted\n",
    "    - Cluster size where the SSE starts to level off is optimal (don't want too low as that may mean overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f158f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(data, max_k):\n",
    "    \"\"\"\n",
    "    Iterate through each cluster size up to max_k and plot SSE of each clusters\n",
    "    \"\"\"\n",
    "    k_vals = range(2, max_k + 1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    for k in k_vals:\n",
    "        sse.append(\n",
    "            KMeans(n_clusters=k, random_state=520).fit(data).inertia_\n",
    "            \n",
    "            #MiniBatchKMeans(\n",
    "            #    n_clusters=k, init_size=1024, batch_size=2040, random_state=20\n",
    "            #).fit(data).inertia_\n",
    "        )\n",
    "        print(f\"Fitted {k} clusters!\")\n",
    "    \n",
    "    \n",
    "    # Plot the graph\n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(k_vals, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centres')\n",
    "    ax.set_xticks(k_vals)\n",
    "    ax.set_xticklabels(k_vals)\n",
    "    ax.set_ylabel('SSE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcrl2000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "061547610e8fceca887a10faa2400f70de22abf81f0fbff289301edcce13ef23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
