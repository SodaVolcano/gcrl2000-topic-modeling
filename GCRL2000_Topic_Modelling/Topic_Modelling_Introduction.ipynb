{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling Overview\n",
    "\n",
    "*This notebook provides an overview of various models and techniques commonly used in [topic modelling](https://en.wikipedia.org/wiki/Vector_space_model).* We will be using the [`sklearn`](https://en.wikipedia.org/wiki/Vector_space_model) library to demonstrate the techniques and models discussed, though they are also present in other libraries like [`gensim`](https://en.wikipedia.org/wiki/Vector_space_model).\n",
    "\n",
    "> **Author's note**: This notebook contains everything I've learnt while undertaking my GCRL2000 placement. I've tried rephrasing the information in a digestable form (to me at least) but do note that some information may be inaccurate or wrongly interpreted (don't sue me). Hope this is useful :D\n",
    ">\n",
    "> \t-Tin Chi Pang (23301921)\n",
    "\n",
    "**Run the code below before you execute any other cells**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"dog bites man\", \"man bites dog\", \"dog bites meat and dog eats meat\"]\n",
    "\n",
    "def print_matrix(pretext, matrix):\n",
    "    print(pretext)\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(f'Document {i}:\\t[', end='')\n",
    "        for j in range(matrix.shape[1]):\n",
    "            print(f'{matrix[i, j]:.3f}', end='')\n",
    "            if j + 1 != matrix.shape[1]:\n",
    "                print(f',\\t', end='')\n",
    "        print(']')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- [Introduction - What is Topic Modelling?](#introduction)\n",
    "    - [Key Definitions](#definitions)\n",
    "    - [What is a \"Topic\"?](#topic)\n",
    "- [Vectorisation](#vectorisation)\n",
    "    - [Vector Space Model](#vector-space)\n",
    "    - [Document-Term Matrix](#document-term)\n",
    "    - [Vectorisation Techniques](#vectorisation-techniques)\n",
    "        - [Bag-of-Word (BoW)](#bow)\n",
    "        - [Bag-of-$n$-Grams](#bog)\n",
    "        - [TF-IDF](#tfidf)\n",
    "- [Topic Models](#models)\n",
    "    - [Latent Dirichlet Allocation (LDA)](#lda)\n",
    "    - [Non-Negative Matrix Factorisation (NMF)](#nmf)\n",
    "- [Dimensionality Reduction](#dimensionality-reduction)\n",
    "    - [Principal Component Analysis (PCA)](#pca)\n",
    "    - [t-SNE](#tsne)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## 1. Introduction - What is Topic Modelling?\n",
    "> **Topic modelling** is a subfield of **Natural Language Processing** (NLP) concerned with finding abstract *topics* in a collection of documents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"definitions\"></a>\n",
    "### Key Definitions\n",
    "| Key term   | Definition                                                                                                |\n",
    "| ---------- | --------------------------------------------------------------------------------------------------------- |\n",
    "| Document   | A single text; e.g. a sentence, tweet, essay, or a Wikipedia article.                                     |\n",
    "| Corpus     | A collection of documents.                                                                                |\n",
    "| Term       | Individual words or phrases in the vocabulary.                                                            |\n",
    "| Vocabulary | A set of all relevant terms throughout the entire corpus (excludes stop words and other irrelevant terms) |\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topic\"></a>\n",
    "### What is a \"Topic\"?\n",
    "A **topic** is a set of words, that together suggest a shared theme. For example, consider the following set:\n",
    "$$\n",
    "\\set{\\text{human, nucleus, DNA, codon, adenine, cell}}\n",
    "$$\n",
    "We can infer that the topic may be \"genetics\", or \"human biology\" etc. - note that we must interpret and assign the actual topic manually by looking at the words. Often we may need to look into the documents themselves to gain a better insight on what the actual topic is.\n",
    "\n",
    "A topic can also be described as a *probability distribution* over the vocabulary. Say that we have some documents about \"photosynthesis\" and some documents about the \"effect of chocolate on pets\" in our corpus. We've removed all stop words like \"the\" but forgot to remove the word \"and\".\n",
    "$$\n",
    " \\text{vocab} = \\set{\\text{dog, chloroplast, photosynthesis, cat, leaf, glucose, chocolate, and}}\n",
    "$$\n",
    "Intuitively we know that a document about \"photosynthesis\" will have more occurrence of terms like \"leaf\" or \"chloroplast\" - hence we may have a probability distribution like this for that topic:\n",
    "\n",
    "![](attachments/1.png)\n",
    "\n",
    "\n",
    "We can then define topic as the set of terms from the vocabulary with high probability of occuring. In that case, the topic \"photosynthesis\" will be the set:\n",
    "$$\n",
    " \\text{photosynthesis}_\\text{topic} = \\set{\\text{chloroplast, photosynthesis, leaf, glucose}}\n",
    "$$\n",
    "\n",
    "Topic modelling is thus about finding these hidden sets of words with shared theme within a given corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vectorisation\"></a>\n",
    "## 2. Vectorisation\n",
    "Before applying Machine Learning and statistical models onto the textual data, we must first convert them to numerical representations. For example, image data is often represented as a matrix of numbers, where each number represents the RGB value of a pixel in the image. Unfortunately, texts are a little more complicated than other forms of data to convert."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vector-space\"></a>\n",
    "### Vector Space Model\n",
    "The [**vector space model**](https://en.wikipedia.org/wiki/Vector_space_model) is an algebraic model for representing texts, where documents are represented as a vector (list) of numbers (hence the name *vectorisation*). This number can simply be the number of times a term has occured in said document, or some other metric. \n",
    "\n",
    "Typically, each term in the vocabulary is assigned an ID. The $i$-th number in a vector is the numerical representation for the $i$-th term in the dictionary (please see the [Bag-of-Word section](#bow) for an example).\n",
    "\n",
    "This way, each document can be thought of as a point in a $n$-dimensional [vector space](https://en.wikipedia.org/wiki/Vector_space) where $n$ is the number of terms in the vocabulary. Documents with similar terms will be closer in the vector space than other documents with different terms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"document-term\"></a>\n",
    "### Document-Term Matrix\n",
    "A **document-term matrix** for a corpus of $m$ documents and vocabulary of size $n$, is a $m \\times n$ matrix (left). each number in the matrix is the numeric representation for a term in a document. This is just a convenient way of representing a list of document vectors (right).\n",
    "$$\\text{document-term matrix} = \\begin{Bmatrix} \n",
    "& w_1 & w_2 & \\dots & w_n\\\\\n",
    "D_1 & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "D_2 & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "D_3 & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "& & \\vdots \\\\\n",
    "D_m & \\dots & \\dots & \\dots & \\dots \n",
    "\\end{Bmatrix}\\ \\  \\equiv \\ \\ \n",
    "\\begin{matrix}\n",
    "D_1 = [\\dots,\\  \\dots,\\  \\dots, \\ \\dots] \\\\\n",
    "D_2 = [\\dots,\\  \\dots,\\  \\dots, \\ \\dots] \\\\\n",
    "D_3 = [\\dots,\\  \\dots,\\  \\dots, \\ \\dots] \\\\\n",
    "\\vdots \\\\\n",
    "D_m = [\\dots,\\  \\dots,\\  \\dots, \\ \\dots] \\\\\n",
    "\\end{matrix}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vectorisation-techniques\"></a>\n",
    "### Vectorisation Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bow\"></a>\n",
    "#### Bag-of-Word (BoW)\n",
    "The [Bag-of-Word model](https://en.wikipedia.org/wiki/Bag-of-words_model) represent each document as a list of frequency counts for each term in the vocabulary.\n",
    "\n",
    "- Formally...\n",
    "\t- Given a vocabulary $V$ (with $|V|$ denoting the number of terms in $V$) with each term in it assigned a unique ID\n",
    "\t- a document $d_j$ is a vector of size $|V|$ that stores the number of times a term in $V$ occured in $d_j$\n",
    "\n",
    "For example, consider the following corpus:\n",
    "| Document | Text           |\n",
    "| -------- | -------------- |\n",
    "| D1       | dog bites man |\n",
    "| D2       | man bites dog |\n",
    "| D3       | dog bites meat and dog eats meat|\n",
    "- $V = \\set{\\text{dog, bites, man, eats, meat, and}}$\n",
    "\t- each term is assigned an ID ranging from 1 to 6\n",
    "- Then, D1 can be represented as a 6D vector where the $i$-th number represent how many times the $i$-th term in $V$ occured in D1:\n",
    "$$\\text{D1} = [1, 1, 1, 0, 0, 0]$$\n",
    "- Applying this to all documents, we end up with the following document-term matrix:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "V & =& \\{\\text{dog}, & \\text{bites}, &  \\text{man},& \\text{eats}, & \\text{meat}, &  \\text{and}\\} \\\\\n",
    "\\text{ID} & =& \\{1,& 2,& 3,& 4,& 5,& 6\\} \\\\\n",
    "\\\\\n",
    "D_1 & =& [1,& 1,& 1,& 0,& 0,&  0] \\\\\n",
    "D_2 & =& [1,& 1,& 1,& 0,& 0,& 0] \\\\\n",
    "D_3 & =& [2,& 1,& 0,& 1,& 1,& 1] \\\\\n",
    "\\end{matrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: ['dog bites man', 'man bites dog', 'dog bites meat and dog eats meat']\n",
      "Vocabulary: ['and' 'bites' 'dog' 'eats' 'man' 'meat']\n",
      "Document-terms matrix (BoW)\n",
      "Document 0:\t[0.000,\t1.000,\t1.000,\t0.000,\t1.000,\t0.000]\n",
      "Document 1:\t[0.000,\t1.000,\t1.000,\t0.000,\t1.000,\t0.000]\n",
      "Document 2:\t[1.000,\t1.000,\t2.000,\t1.000,\t0.000,\t2.000]\n"
     ]
    }
   ],
   "source": [
    "# BoW in sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectoriser = CountVectorizer()\n",
    "doc_term_matrix = vectoriser.fit_transform(corpus)\n",
    "\n",
    "print(f'Corpus: {corpus}')\n",
    "print(f'Vocabulary: {vectoriser.get_feature_names_out()}')\n",
    "print_matrix(\"Document-terms matrix (BoW):\", doc_term_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bog\"></a>\n",
    "#### Bag-of-$n$-Grams\n",
    "The bag-of-$n$-grams model is the exact same as BoW, differing in only that each term in the vocabulary can consist of more than one word. These phrases consisting of $n$ words are called [$n$-grams](https://en.wikipedia.org/wiki/N-gram). For example, \"dog\", \"cat\", \"chocolate\" are 1-grams (AKA *unigrams*), \"red dog\", \"black cat\", \"white chocolate\" are 2-grams (AKA *bigrams*) and so forth. This captures word-order to some degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: ['dog bites man', 'man bites dog', 'dog bites meat and dog eats meat']\n",
      "Vocabulary: ['and dog' 'bites dog' 'bites man' 'bites meat' 'dog bites' 'dog eats'\n",
      " 'eats meat' 'man bites' 'meat and']\n",
      "Document-terms matrix (BoG):\n",
      "Document 0:\t[0.000,\t0.000,\t1.000,\t0.000,\t1.000,\t0.000,\t0.000,\t0.000,\t0.000]\n",
      "Document 1:\t[0.000,\t1.000,\t0.000,\t0.000,\t0.000,\t0.000,\t0.000,\t1.000,\t0.000]\n",
      "Document 2:\t[1.000,\t0.000,\t0.000,\t1.000,\t1.000,\t1.000,\t1.000,\t0.000,\t1.000]\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-n-grams in sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectoriser = CountVectorizer(ngram_range=(2, 2))\n",
    "doc_term_matrix = vectoriser.fit_transform(corpus)\n",
    "\n",
    "print(f'Corpus: {corpus}')\n",
    "print(f'Vocabulary: {vectoriser.get_feature_names_out()}')\n",
    "print_matrix(\"Document-terms matrix (BoG):\", doc_term_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tfidf\"></a>\n",
    "#### TF-IDF\n",
    "Stands for \"*Term Frequency-Inverse-Document-Frequency*\", [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) uses a weighting that reflects how important a term is to a document (as opposed to merely being a frequency count like in BoW). This TF-IDF weight value is high if a term only occurs in a small number of documents, with the idea that rarer terms like \"scoliosis\" are more likely to be meaningful as opposed to more commonly-used terms like \"medical\". \n",
    "\n",
    "For some term $t$ and some document $d$ in corpus $D$, the TF-IDF score is calculated as the product of TF and IDF:\n",
    "$$\\text{tf-idf}(t, d, D) = \\text{tf}(t, d) \\cdot \\text{idf}(t, D)$$\n",
    "$$\\text{tf}(t, d) = \\frac{\\text{number of times }t \\text{ occured in }d}{\\text{total frequencies of terms in }d} = \\frac{\\text{count}(t, d)}{\\sum\\limits_{t' \\in d} \\text{count}(t', d)}$$\n",
    "$$\n",
    "\\text{idf}(t, D) = \\log \\frac{\\text{number of documents}}{\\text{number of documents that } t \\text{ occurs in}} = \\frac{|D|}{|\\set{d \\in D: t \\in d}|}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: ['dog bites man', 'man bites dog', 'dog bites meat and dog eats meat']\n",
      "Vocabulary: ['and' 'bites' 'dog' 'eats' 'man' 'meat']\n",
      "Document-terms matrix (TF-IDF):\n",
      "Document 0:\t[0.000,\t0.523,\t0.523,\t0.000,\t0.673,\t0.000]\n",
      "Document 1:\t[0.000,\t0.523,\t0.523,\t0.000,\t0.673,\t0.000]\n",
      "Document 2:\t[0.359,\t0.212,\t0.424,\t0.359,\t0.000,\t0.719]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF in sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectoriser = TfidfVectorizer()\n",
    "doc_term_matrix = vectoriser.fit_transform(corpus)\n",
    "\n",
    "print(f'Corpus: {corpus}')\n",
    "print(f'Vocabulary: {vectoriser.get_feature_names_out()}')\n",
    "print_matrix(\"Document-terms matrix (TF-IDF):\", doc_term_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "## 3. Topic Models\n",
    "After vectorising the textual data, various models can then be applied on the resulting document-term matrix to possibly discover the topics in the corpus (often using unsupervised learning). Unfortunately the number of topics must be specified, and the hyperparameters of the model must be tuned experimentally to find interpretable topics from the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lda\"></a>\n",
    "### Latent Dirichlet Allocation (LDA)\n",
    "[LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is the most commonly used statistical model in topic modelling, where topics are described as *probability distribution* over the vocabulary (see ['What is a \"Topic\"?'](#topic)).\n",
    "\n",
    "LDA works by assuming that documents is a mixture of topics and can be *generated* by the following steps:\n",
    "1. Choose how many terms to write into the document\n",
    "2. Select the mixture of topics that this document should cover (e.g. 40% topic X, 60% topic Y, 0% topic Z)\n",
    "3. Then, to write each term into the document...\n",
    "\n",
    "\t4. Choose a topic from the mixture (e.g. we chose topic X with 40% chance)\n",
    "\t5. Choose a term from that selected topic - this is the term we will write into the document (e.g. say, we chose the term \"cat\" have 15% chance of occuring in topic X)\n",
    "\n",
    "LDA will attempt to reverse this process starting from the already-written documents to identify the hidden topics (\"latent\"). Say we have 3 topics, and we plot them on a 2D graph. Each document is then allocated onto the graph with the position indicating how much of the document is of specific topic.\n",
    "\n",
    "![](attachments/2.png)\n",
    "\n",
    "Given a document-term matrix $M$ of shape $(m, n)$ and specifying $k$ topics, LDA will factorise it into 2 submatrices which can be used to understand the topic structure of the corpus.\n",
    "- $M_1$ = *document-topic matrix*, dimension $(m, k)$ \n",
    "- $M_2$ = *topic-term matrix*, dimension $(k, n)$\n",
    "For example, say we have $M$ = \n",
    "\n",
    "![](attachments/3.png)\n",
    "\n",
    "$M_1$ and $M_2$ respectively =\n",
    "\n",
    "![](attachments/4.png)\n",
    "\n",
    "> **Author's note**: I don't know the specifics on how LDA reverses the steps, I just know it produces 2 matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA in sklearn, using BoW document-term matrix\n",
    "# https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "vectoriser = CountVectorizer()\n",
    "doc_term_matrix = vectoriser.fit_transform(corpus)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=2)\n",
    "\n",
    "doc_topic_matrix = lda.fit_transform(doc_term_matrix)\n",
    "topic_term_matrix = lda.components_\n",
    "\n",
    "#TODO: visualise using nlp functions I wrote"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nmf\"></a>\n",
    "### Non-Negative Matrix Factorisation (NMF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dimensionality-reduction\"></a>\n",
    "## 4. Dimensionality Reduction\n",
    "During analysis, if we have $n$ terms in our vocabulary, then each document is represented as a $n$-dimensional vector. Often than not $n$ will be an increadibly large number, making it difficult to plot and visualise the individual document in the vector space. Hence, we must use [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) techniques to reduce the dimensions without altering the underlying structure of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pca\"></a>\n",
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tsne\"></a>\n",
    "### t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcrl2000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "061547610e8fceca887a10faa2400f70de22abf81f0fbff289301edcce13ef23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
