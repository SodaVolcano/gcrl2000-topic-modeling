{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "- This notebook provides the code to quickly configure settings, train and then save a model, and then loading it to display its results. If you already have a trained model, execute the cells from the middle of the notebook (the section will be labelled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these settings\n",
    "config = {\n",
    "    # Name to save model as, change the first 3 letters to 'lda' or 'nmf' will use the respective model\n",
    "    'model_name': 'lda_model_1.pckl',   \n",
    "    # Whether to drop all tweets that contains URLs, TWITTER DATASET ONLY\n",
    "    'drop_linked_tweets': True,\n",
    "    # Minimum document frequency cut-off\n",
    "    'min_df': 5,                        \n",
    "    # Maximum document frequency cut-off\n",
    "    'max_df': 0.95,                      \n",
    "    # Range of number words a term can have\n",
    "    'ngram_range' : (1, 2),             \n",
    "    # Dataset to read & train from\n",
    "    'corpus': './dataset/twitter.csv',     \n",
    "    # Column name containing the texts, load_twitter_csv() will be used if this is \"tweet\", else normal pandas.DataFrame constructor is called\n",
    "    'df_col_name': 'tweet',             \n",
    "    # Set to None to disable and read all rows\n",
    "    'nrows': None,                     \n",
    "    # Vectoriser to use, 'tfidf' or 'bow'\n",
    "    'vectoriser': 'tfidf',              \n",
    "    # Number of topics to find\n",
    "    'n_topics': 15,                      \n",
    "    # Max iterations for model training\n",
    "    'max_iter': 1000                      \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import *\n",
    "import pandas as pd\n",
    "\n",
    "resources = ['corpora/stopwords', 'corpora/wordnet',\n",
    "             'taggers/averaged_perceptron_tagger']\n",
    "\n",
    "check_nltk_resources(resources)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words.extend(\n",
    "    list(string.punctuation) + [\n",
    "        'would', 'could', 'get', 'want', 'he', 'twitter', 'elon', 'musk', \n",
    "        'well', 'need', 'come', 'really', 'take', 'say', 'go', 'use', 'make',\n",
    "        'know', 'think', 'deal'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "\n",
    "# Load the CSV file using appropriate function\n",
    "if config['df_col_name'] != 'tweet':\n",
    "    df = pd.read_csv(config['corpus'], nrows=config['nrows'])\n",
    "else:\n",
    "    df = load_twitter_csv(\n",
    "        config['corpus'], \n",
    "        do_preprocess=False, \n",
    "        nrows=config['nrows'], \n",
    "        drop_linked_tweets=config['drop_linked_tweets']\n",
    "    )\n",
    "\n",
    "\n",
    "preprocess_df(df, txt_col=config['df_col_name'], stop_words=stop_words,\n",
    "              inplace=True)\n",
    "corpus = df[f\"{config['df_col_name']}_preprocessed\"]\n",
    "\n",
    "\n",
    "# Use specified vectoriser\n",
    "if config['vectoriser'] == 'tfidf':\n",
    "    config['vectoriser'] = TfidfVectorizer(\n",
    "        min_df=config['min_df'], max_df=config['max_df'],\n",
    "        ngram_range=config['ngram_range']\n",
    "    )\n",
    "elif config['vectoriser'] == 'bow':\n",
    "    config['vectoriser'] = CountVectorizer(\n",
    "        min_df=config['min_df'], max_df=config['max_df'],\n",
    "        ngram_range=config['ngram_range']\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Invalid vectoriser name '{config['vectoriser']}'\")\n",
    "\n",
    "\n",
    "doc_term = config['vectoriser'].fit_transform(corpus)\n",
    "\n",
    "\n",
    "# Use specified model\n",
    "match config['model_name'][:3]:\n",
    "    case 'lda':\n",
    "        model = LatentDirichletAllocation(\n",
    "            n_components=config['n_topics'],\n",
    "            max_iter=config['max_iter'],\n",
    "            verbose=1\n",
    "        )\n",
    "    case 'nmf':\n",
    "        model = NMF(\n",
    "            n_components=config['n_topics'],\n",
    "            max_iter=config['max_iter'],\n",
    "            verbose=1\n",
    "        )\n",
    "    case _:\n",
    "        raise ValueError(f\"Invalid model name '{config['model_name']}'\")\n",
    "\n",
    "\n",
    "model.info = config\n",
    "model.info['vectoriser'].get_feature_names = \\\n",
    "    model.info['vectoriser'].get_feature_names_out\n",
    "\n",
    "\n",
    "doc_topics = model.fit(doc_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "import pickle\n",
    "\n",
    "save_path = f\"./models/{config['model_name']}\"\n",
    "\n",
    "with open(save_path, \"wb\") as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Run the cells from this point onwards to load in a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this setting\n",
    "\n",
    "#model_path = \"./models/lda_model_1.pckl\"\n",
    "model_path = save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved model\n",
    "import pickle\n",
    "from nlp import *\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "with open(model_path, \"rb\") as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "print(f\"Loaded in model with info: {model.info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import *\n",
    "import pandas as pd\n",
    "\n",
    "resources = ['corpora/stopwords', 'corpora/wordnet',\n",
    "             'taggers/averaged_perceptron_tagger']\n",
    "\n",
    "check_nltk_resources(resources)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words.extend(\n",
    "    list(string.punctuation) + [\n",
    "        'would', 'could', 'get', 'want', 'he', 'twitter', 'elon', 'musk', \n",
    "        'well', 'need', 'come', 'really', 'take', 'say', 'go', 'use', 'make',\n",
    "        'know', 'think', 'deal'\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "if model.info['df_col_name'] != 'tweet':\n",
    "    df = pd.read_csv(model.info['corpus'], nrows=model.info['nrows'])\n",
    "else:\n",
    "    df = load_twitter_csv(\n",
    "        model.info['corpus'], \n",
    "        do_preprocess=False,\n",
    "        nrows=model.info['nrows'], \n",
    "        drop_linked_tweets=model.info['drop_linked_tweets']\n",
    "    )\n",
    "\n",
    "preprocess_df(\n",
    "    df, txt_col=model.info['df_col_name'], \n",
    "    stop_words=stop_words,\n",
    "    inplace=True\n",
    ")\n",
    "corpus = df[f\"{model.info['df_col_name']}_preprocessed\"]\n",
    "\n",
    "doc_terms = model.info['vectoriser'].transform(corpus)\n",
    "\n",
    "doc_topics = model.transform(doc_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_topic_terms(\n",
    "    topic_terms=model.components_,\n",
    "    vocab=model.info['vectoriser'].get_feature_names_out(),\n",
    "    n_words=50,\n",
    "    dump_to_file=True,\n",
    "    dump_file_name=\"./model_training_topic_terms_dump.txt\"\n",
    ")\n",
    "\n",
    "print_doc_topics(\n",
    "    doc_topics=doc_topics,\n",
    "    corpus=df[model.info['df_col_name']],\n",
    "    n_docs=100,\n",
    "    dump_to_file=True,\n",
    "    dump_file_name=\"./model_training_doc_topics_dump.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "plot_document_matrix(doc_topics, dimension=2, decomposer='tsne')\n",
    "plt.show()\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(model, doc_terms, model.info['vectoriser'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcrl2000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "061547610e8fceca887a10faa2400f70de22abf81f0fbff289301edcce13ef23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
