{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b9c1b9",
   "metadata": {},
   "source": [
    "# GCRL2000 - NLP Functions\n",
    "- This notebook contains various functions for processing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e09bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import TweetTokenizer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "default_stopwords = stopwords.words('english')\n",
    "\n",
    "default_stopwords.extend(\n",
    "    list(string.punctuation) + ['would', 'could', 'get',\n",
    "                                'want', 'he', 'twitter']\n",
    ")\n",
    "\n",
    "default_tokeniser = TweetTokenizer()\n",
    "default_lemmatiser = WordNetLemmatizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "decb2595",
   "metadata": {},
   "source": [
    "- `stopwords` contain words to remove from the tweets, e.g., \"the\"\n",
    "- `wordnet` is used by the `WordNetLemmatiser`\n",
    "- `averaged_perceptron_tagger` is used by `pos_tagger()`, it tags a word into nouns, verbs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e959dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_nltk_downloads() -> None:\n",
    "    \"\"\"\n",
    "    Download the necessary resources for nltk, such as stopwords\n",
    "    \"\"\"\n",
    "    resources = ['corpora/stopwords', 'corpora/wordnet',\n",
    "                 'taggers/averaged_perceptron_tagger']\n",
    "    for resource in resources:\n",
    "        # Find .zip file instead since nltk have problem unzipping files\n",
    "        try:\n",
    "            nltk.find(f'{resource}.zip')\n",
    "        except LookupError:\n",
    "            nltk.download(resource.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53c32a",
   "metadata": {},
   "source": [
    "- Regex explanation:\n",
    "    - `r\"(@[A-Za-z0-9_]+)\"` will match any string, starting with \"@\", containing letters, numbers, or underscore\n",
    "    - `r\"^http.+?|(\\w+:\\/\\S+)\"` match strings starting with \"http\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210756ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_twitter_csv(data_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load in a csv file produce by Twitter scraper, return cleaned DataFrame\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        data_file,\n",
    "        index_col=0,\n",
    "        usecols=['conversation_id', 'created_at', 'user_id', 'tweet',\n",
    "                 'language']\n",
    "    )\n",
    "    # Only consider English tweets, ignore neutral language\n",
    "    df.query('language == \"en\"', inplace=True)\n",
    "    df.drop(columns=['language'], inplace=True)\n",
    "    df.dropna(subset=['tweet'], inplace=True)\n",
    "\n",
    "    # Clean tweet texts\n",
    "    df['tweet'] = \\\n",
    "        df['tweet'].apply(\n",
    "            lambda txt: re.sub(\n",
    "                r\"(@[A-Za-z0-9_]+)|\"        # Match mentions\n",
    "                r\"^http.+?|(\\w+:\\/\\S+)\",    # Match urls\n",
    "                '',\n",
    "                txt\n",
    "            ).lower()\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d8d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and turn word into lemmatised form\n",
    "def furnish(\n",
    "        text: str, \n",
    "        tokeniser=default_tokeniser, \n",
    "        lemmatiser=default_lemmatiser, \n",
    "        stop_words: list = default_stopwords\n",
    "):\n",
    "    final_text = []\n",
    "    for word, tag in pos_tag(tokeniser.tokenize(text)):\n",
    "        # Tag word as verb, nouns, etc, improves lemmatiser accuracy\n",
    "        tag = tag.lower()[0]\n",
    "        tag = tag if tag in ['a', 'r', 'n', 'v'] else None\n",
    "        if tag:\n",
    "            word = lemmatiser.lemmatize(word, tag)\n",
    "        else:\n",
    "            word = lemmatiser.lemmatize(word)\n",
    "        if word not in stop_words:\n",
    "            final_text.append(word)\n",
    "    return ' '.join(final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cf94b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_n_grams(\n",
    "        df: pd.DataFrame, new_col_name=\"tweet_n_gram\", min_len=1, max_len=3\n",
    "):\n",
    "    df[new_col_name] = df['tweet'].apply(lambda x: list(\n",
    "        nltk.everygrams(x.split(' '), min_len=min_len, max_len=max_len))\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99046d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_words(decomposer, vectoriser, n_words):\n",
    "    for i, topic in enumerate(decomposer.components_):\n",
    "        print(f'Top {n_words} words for topic #{i}:')\n",
    "        print([vectoriser.get_feature_names_out()[i]\n",
    "               for i in topic.argsort()[-n_words:]])\n",
    "        print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcrl2000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "061547610e8fceca887a10faa2400f70de22abf81f0fbff289301edcce13ef23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
