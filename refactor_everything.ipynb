{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains everything I've done so far, categorised for splitting into different files later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "resources = ['corpora/stopwords', 'corpora/wordnet',\n",
    "             'taggers/averaged_perceptron_tagger']\n",
    "\n",
    "def check_nltk_resources(resources: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Download the necessary resources for nltk, such as stopwords\n",
    "    \"\"\"\n",
    "    for resource in resources:\n",
    "        # Find .zip file instead since nltk have problem unzipping files\n",
    "        try:\n",
    "            nltk.find(f'{resource}.zip')\n",
    "        except LookupError:\n",
    "            nltk.download(resource.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Literal\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer, WordNetLemmatizer, pos_tag\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "        df: pd.DataFrame,\n",
    "        txt_col=\"tweet\",   # Specify column to clean\n",
    "        stop_words=stopwords.words('english'),\n",
    "        tokeniser=TweetTokenizer(),\n",
    "        lemmatiser=WordNetLemmatizer(),\n",
    "        filter_regex=r'',   # Used to filter matches\n",
    "        remove_punct=True,\n",
    "        remove_mentions=True,\n",
    "        remove_hashtags=True,\n",
    "        remove_urls=True,\n",
    "        casing: Literal[\"lower\", \"upper\", None] = 'lower'\n",
    "):\n",
    "    filter_regex = build_regex(\n",
    "        filter_regex, remove_mentions, remove_hashtags, remove_urls\n",
    "    )\n",
    "\n",
    "    if remove_punct:\n",
    "        stop_words.extend(string.punctuation)\n",
    "\n",
    "    casing_func = lambda x: x   # Don't change casing\n",
    "    if casing == 'lower':\n",
    "        casing_func = str.lower\n",
    "    elif casing == 'upper':\n",
    "        casing_func = str.upper\n",
    "    elif casing is not None:\n",
    "        raise ValueError(\n",
    "            \"Parameter 'casing' can only have value: 'lower', 'upper', or None\"\n",
    "        )\n",
    "        \n",
    "    # Apply regex filter - remove all matched texts\n",
    "    df[txt_col] = df[txt_col].apply(\n",
    "        lambda txt: casing_func(re.sub(filter_regex, '', txt))\n",
    "    )\n",
    "    df[txt_col] = df[txt_col].apply(\n",
    "        furnish, args=(tokeniser, lemmatiser, stop_words)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_regex(\n",
    "        regex: str, remove_mentions: bool, \n",
    "        remove_hashtags: bool, remove_urls: bool\n",
    "):\n",
    "    regex_mentions = r\"(@[A-Za-z0-9_]+)\"\n",
    "    regex_hashtags = r\"(#[A-Za-z0-9_]+)\"\n",
    "    regex_urls = \\\n",
    "        r\"(https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.\" + \\\n",
    "        r\"[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()!@:%_\\+.~#?&\\/\\/=]*))\"\n",
    "    \n",
    "    # Add '|\" onto the end if regex string is not empty\n",
    "    add_delim = lambda r: f\"{r}{r'|' if len(r) != 0 else r''}\"\n",
    "    \n",
    "    # Keep as list to reduce redundant code\n",
    "    regexes = [regex_mentions, regex_hashtags, regex_urls]\n",
    "    add_regex = [remove_mentions, remove_hashtags, remove_urls]\n",
    "    for i in range(len(regexes)):\n",
    "        regex = add_delim(regex)\n",
    "        if add_regex[i]:\n",
    "            regex += regexes[i]\n",
    "   \n",
    "    return regex\n",
    "\n",
    "    \n",
    "# Remove stopwords and turn word into lemmatised form\n",
    "def furnish(text: str, tokeniser, lemmatiser, stop_words) -> str:\n",
    "    final_text = []\n",
    "    for word, tag in pos_tag(tokeniser.tokenize(text)):\n",
    "        # Tag word as verb, nouns, etc, improves lemmatiser accuracy\n",
    "        tag = tag.lower()[0]\n",
    "        if tag in ['a', 'r', 'n', 'v']:\n",
    "            word = lemmatiser.lemmatize(word, tag)\n",
    "        else:\n",
    "            word = lemmatiser.lemmatize(word)\n",
    "\n",
    "        if word not in stop_words:\n",
    "            final_text.append(word)\n",
    "\n",
    "    return ' '.join(final_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_twitter_csv(\n",
    "        file_path: str,\n",
    "        usecols=['conversation_id', 'tweet', 'language'],\n",
    "        index_col=0,\n",
    "        eng_only=True,\n",
    "        do_preprocess=True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create (preprocessed) DataFrame from csv file containing only English tweets\n",
    "\n",
    "    csv file MUST have a column named 'tweet'.\n",
    "    \n",
    "    Use preprocess() for more customisation.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, index_col=index_col, usecols=usecols)\n",
    "    \n",
    "    # Filter out non-English tweets\n",
    "    if eng_only:\n",
    "        df.query('language == \"en\"', inplace=True)\n",
    "        df.drop(columns=['language'], inplace=True)\n",
    "\n",
    "    df.dropna(subset=['tweet'], inplace=True)\n",
    "\n",
    "    if do_preprocess:\n",
    "        df = preprocess(df)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorisation\n",
    "- Converts textual data into numeric form as vectors, producing a **term matrix**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-Of-Word (BoW)\n",
    "- Given a vocabulary, count up occurrence of each term in all documents and produce vector with those frequencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sklearn` - `CountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `gensim` - `Dictionary`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF - Term frequency-Inverse Document Frequency\n",
    "- Vectoriser that take in words and produce a matrix containing weighting for each word\n",
    "- the weight reflect how *important* a word is to a *document* in a collection\n",
    "- For each word, the weight is the product of these two below:\n",
    "    - **Term frequency**: How many times the term occurs, calculated for *each* document\n",
    "    $$\\text{tf}(t, d) = \\frac{\\text{raw count of term }t}{\\text{sum of frequency for all terms in document }d}$$\n",
    "    - **Inverse document frequency**: number of documents divided by number of documents the word occured in, scaled logarithmically\n",
    "    $$\\text{idf}(t, D) = \\log \\frac{\\text{total number of documents in collection }D}{\\text{number of documents that term }t \\text{ occured in}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sklearn` - `TfidfVectorizer`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `gensim` - `TfidfModel`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative Matrix Factorisation (NMF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcrl2000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "061547610e8fceca887a10faa2400f70de22abf81f0fbff289301edcce13ef23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
