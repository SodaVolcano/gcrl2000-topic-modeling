{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains everything I've done so far, categorised for splitting into different files later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "def check_nltk_resources(resources: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Download the necessary resources for nltk, such as stopwords\n",
    "    \"\"\"\n",
    "    for resource in resources:\n",
    "        # Find .zip file instead since nltk have problem unzipping files\n",
    "        try:\n",
    "            nltk.find(f'{resource}.zip')\n",
    "        except LookupError:\n",
    "            nltk.download(resource.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Literal\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer, WordNetLemmatizer, pos_tag\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "        df: pd.DataFrame,\n",
    "        txt_col=\"tweet\",   # Specify column to clean\n",
    "        stop_words=stopwords.words('english'),\n",
    "        tokeniser=TweetTokenizer(),\n",
    "        lemmatiser=WordNetLemmatizer(),\n",
    "        filter_regex=r'',   # Used to filter matches\n",
    "        remove_punct=True,\n",
    "        remove_mentions=True,\n",
    "        remove_hashtags=True,\n",
    "        remove_urls=True,\n",
    "        casing: Literal[\"lower\", \"upper\", None] = 'lower'\n",
    "):\n",
    "    filter_regex = build_regex(\n",
    "        filter_regex, remove_mentions, remove_hashtags, remove_urls\n",
    "    )\n",
    "    if remove_punct:\n",
    "        stop_words.extend(string.punctuation)\n",
    "\n",
    "    casing_func = lambda x: x   # Don't change casing\n",
    "    if casing == 'lower':\n",
    "        casing_func = str.lower\n",
    "    elif casing == 'upper':\n",
    "        casing_func = str.upper\n",
    "    elif casing is not None:\n",
    "        raise ValueError(\n",
    "            \"Parameter 'casing' can only have value: 'lower', 'upper', or None\"\n",
    "        )\n",
    "        \n",
    "    # Apply regex filter - remove all matched texts\n",
    "    df[txt_col] = df[txt_col].apply(\n",
    "        lambda txt: casing_func(re.sub(filter_regex, '', txt))\n",
    "    )\n",
    "    df[txt_col] = df[txt_col].apply(\n",
    "        furnish, args=(tokeniser, lemmatiser, stop_words)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_regex(\n",
    "        regex: str, remove_mentions: bool, \n",
    "        remove_hashtags: bool, remove_urls: bool\n",
    "):\n",
    "    regex_mentions = r\"(@[A-Za-z0-9_]+)\"\n",
    "    regex_hashtags = r\"(#[A-Za-z0-9_]+)\"\n",
    "    regex_urls = \\\n",
    "        r\"(https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.\" + \\\n",
    "        r\"[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()!@:%_\\+.~#?&\\/\\/=]*))\"\n",
    "    \n",
    "    # Add '|\" onto the end if regex string is not empty\n",
    "    add_delim = lambda r: f\"{r}{r'|' if len(r) != 0 else r''}\"\n",
    "    \n",
    "    # Keep as list to reduce redundant code\n",
    "    regexes = [regex_mentions, regex_hashtags, regex_urls]\n",
    "    add_regex = [remove_mentions, remove_hashtags, remove_urls]\n",
    "    for i in range(len(regexes)):\n",
    "        regex = add_delim(regex)\n",
    "        if add_regex[i]:\n",
    "            regex += regexes[i]\n",
    "   \n",
    "    return regex\n",
    "\n",
    "    \n",
    "# Remove stopwords and turn word into lemmatised form\n",
    "def furnish(text: str, tokeniser, lemmatiser, stop_words) -> str:\n",
    "    final_text = []\n",
    "    for word, tag in pos_tag(tokeniser.tokenize(text)):\n",
    "        # Tag word as verb, nouns, etc, improves lemmatiser accuracy\n",
    "        tag = tag.lower()[0]\n",
    "        if tag in ['a', 'r', 'n', 'v']:\n",
    "            word = lemmatiser.lemmatize(word, tag)\n",
    "        else:\n",
    "            word = lemmatiser.lemmatize(word)\n",
    "\n",
    "        if word not in stop_words:\n",
    "            final_text.append(word)\n",
    "\n",
    "    return ' '.join(final_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_twitter_csv(\n",
    "        file_path: str,\n",
    "        usecols=['conversation_id', 'tweet', 'language'],\n",
    "        index_col=0,\n",
    "        eng_only=True,\n",
    "        do_preprocess=True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create (preprocessed) DataFrame from csv file containing only English tweets\n",
    "\n",
    "    csv file MUST have a column named 'tweet'.\n",
    "    \n",
    "    Use preprocess() for more customisation.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, index_col=index_col, usecols=usecols)\n",
    "    \n",
    "    # Filter out non-English tweets\n",
    "    if eng_only:\n",
    "        df.query('language == \"en\"', inplace=True)\n",
    "        df.drop(columns=['language'], inplace=True)\n",
    "\n",
    "    df.dropna(subset=['tweet'], inplace=True)\n",
    "\n",
    "    if do_preprocess:\n",
    "        df = preprocess(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources = ['corpora/stopwords', 'corpora/wordnet',\n",
    "             'taggers/averaged_perceptron_tagger']\n",
    "\n",
    "check_nltk_resources(resources)\n",
    "\n",
    "default_stopwords = stopwords.words('english')\n",
    "\n",
    "default_stopwords.extend(\n",
    "    list(string.punctuation) + [\n",
    "        'would', 'could', 'get', 'want', 'he', 'twitter', 'elon', 'musk', \n",
    "        'well', 'need', 'come', 'really', 'take', 'say', 'go', 'use', 'make',\n",
    "        'know', 'think'\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = load_twitter_csv(\"../Dataset/twitter.csv\", do_preprocess=False)\n",
    "df = preprocess(df, stop_words=default_stopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorisation\n",
    "- Converts textual data into numeric form as vectors, producing a **term matrix**\n",
    "- **term matrix** is in this form:\n",
    " \n",
    "|       | term_1 | term_2 | term_3 | ... |\n",
    "| ----- | ------ | ------ | ------ | --- |\n",
    "| doc_1 |        |        |        |     |\n",
    "| doc_2 |        |        |        |     |\n",
    "| doc_3 |        |        |        |     |\n",
    "| ...   |        |        |        |     |\n",
    "\n",
    "\n",
    "- **NOTE**: In `sklearn` vectoriser will produce a term matrix and **store the feature names**\n",
    "- **NOTE**: `gensim`'s models perform transformations, a wrapper is created around the corpus and the transformation occurs on-the-fly\n",
    "    - e.g. `tfidf_model[bow_corpus]` is a TF-IDF wrapper around the BoW vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_1 = [\"dogs and cats are not allowed\", \n",
    "          \"cats and cats are friendly and are allowed\"]\n",
    "corpus = df['tweet']\n",
    "min_df = 10\n",
    "max_df = 0.95"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-Of-Word (BoW)\n",
    "- Given a vocabulary, count up occurrence of each term in all documents and produce vector with those frequencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sklearn` - `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(min_df=min_df, max_df=max_df)\n",
    "bow_matrix1 = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(count_vect.get_feature_names_out())\n",
    "print(bow_matrix1.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag-of-$n$-gram Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(2, 2))\n",
    "bow_matrix1 = count_vect.fit_transform(corpus)\n",
    "\n",
    "print(count_vect.get_feature_names_out())\n",
    "print(bow_matrix1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `gensim` - `Dictionary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Associate each word with a unique integer ID\n",
    "vocab = Dictionary([t.split() for t in corpus])\n",
    "#vocab.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# Use vocab as feature labels, create bag-of-word vectors\n",
    "bow_matrix2 = [vocab.doc2bow(t.split()) for t in corpus]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF - Term frequency-Inverse Document Frequency\n",
    "- Vectoriser that take in words and produce a matrix containing weighting for each word\n",
    "- the weight reflect how *important* a word is to a *document* in a collection\n",
    "- For each word, the weight is the product of these two below:\n",
    "    - **Term frequency**: How many times the term occurs, calculated for *each* document\n",
    "    $$\\text{tf}(t, d) = \\frac{\\text{raw count of term }t}{\\text{sum of frequency for all terms in document }d}$$\n",
    "    - **Inverse document frequency**: number of documents divided by number of documents the word occured in, scaled logarithmically\n",
    "    $$\\text{idf}(t, D) = \\log \\frac{\\text{total number of documents in collection }D}{\\text{number of documents that term }t \\text{ occured in}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sklearn` - `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(min_df=min_df, max_df=max_df)\n",
    "tfidf_matrix1 = tfidf_vect.fit_transform(corpus)\n",
    "\n",
    "print(tfidf_vect.get_feature_names_out())\n",
    "print(tfidf_matrix1.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `gensim` - `TfidfModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Train the model on the corpus, must be BoW\n",
    "tfidf_model = TfidfModel(bow_matrix2)\n",
    "tfidf_matrix2 = tfidf_model[bow_matrix2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis\n",
    "- **TODO**: Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import L"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "term_matrix = bow_matrix1\n",
    "n_topics = 10\n",
    "iterations = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sklearn` - `LatentDirichletAllocation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_sklearn = LatentDirichletAllocation(n_components=n_topics, max_iter=iterations)\n",
    "lda_matrix = lda_sklearn.fit_transform(term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `gensim.models` - `LdaModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "temp = vocab[0] # Load the dictionary, idk why this is necessary???\n",
    "\n",
    "lda_gensim = LdaModel(\n",
    "    corpus=bow_matrix2,\n",
    "    id2word=vocab.id2token,\n",
    "    chunksize=3000,\n",
    "    iterations=iterations,   # Loop over EACH document\n",
    "    num_topics=n_topics,\n",
    "    passes=20,               # Loop over WHOLE corpus\n",
    "    eval_every=1\n",
    ")\n",
    "\n",
    "lda_gensim_topics = lda_gensim.top_topics(bow_matrix2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative Matrix Factorisation (NMF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sklearn` - `NMF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_topics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m NMF\n\u001b[0;32m----> 3\u001b[0m nmf_sklearn \u001b[39m=\u001b[39m NMF(n_components\u001b[39m=\u001b[39mn_topics)\n\u001b[1;32m      4\u001b[0m nmf_term_matrix \u001b[39m=\u001b[39m nmf_sklearn\u001b[39m.\u001b[39mfit_transform(\n\u001b[1;32m      5\u001b[0m     np\u001b[39m.\u001b[39masarray(term_matrix\u001b[39m.\u001b[39mtodense())\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_topics' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_sklearn = NMF(n_components=n_topics)\n",
    "nmf_matrix = nmf_sklearn.fit_transform(\n",
    "    np.asarray(term_matrix.todense())\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sklearn` - `TruncatedSVD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "lsa_sklearn = TruncatedSVD(n_components=n_topics, n_iter=iterations)\n",
    "lsa_matrix = lsa_sklearn.fit_transform(tfidf_matrix1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `gensim` - `LsiModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "lsi_gensim = LsiModel(tfidf_matrix2, id2word=vocab, num_topics=n_topics)\n",
    "lsi_matrix = lsi_gensim[tfidf_matrix2]\n",
    "\n",
    "lsi_gensim.print_topics(n_topics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "279143da",
   "metadata": {},
   "source": [
    "### Optimal Cluster\n",
    "- `KMeans` privides the `inertia_` attribute - sum of square distances of samples to their cluster's centre, i.e. the **SSE**\n",
    "    - SSE is a measure for how fitted the model is to the data, low SSE means the model is very fitted\n",
    "    - Cluster size where the SSE starts to level off is optimal (don't want too low as that may mean overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_optimal_k_cluster_size(data, max_k: int = 20, rand_state=20):\n",
    "    \"\"\"\n",
    "    Iterate through each cluster size up to max_k and plot SSE of each clusters\n",
    "    \"\"\"\n",
    "    k_vals = range(2, max_k + 1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    for k in k_vals:\n",
    "        sse.append(\n",
    "            KMeans(\n",
    "                n_clusters=k, random_state=rand_state, n_init='auto'\n",
    "            ).fit(data).inertia_\n",
    "        )\n",
    "        print(f\"Fitted {k} clusters!\")\n",
    "    \n",
    "    # Plot the graph\n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(k_vals, sse, marker='o')\n",
    "    ax.set_xlabel('Number of Cluster Centroids')\n",
    "    ax.set_xticks(k_vals)\n",
    "    ax.set_xticklabels(k_vals)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title(\"Change in SSE as Number of Clusters Increase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Enable interactive graph\n",
    "%matplotlib widget\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def plot_clusters(\n",
    "    data, \n",
    "    cluster_labels, \n",
    "    n_samples=2000, \n",
    "    decomposer: Literal['pca', 'tsne'] | Any = 'pca',\n",
    "    dimension: Literal[2, 3] = 2\n",
    "):\n",
    "    \"\"\"\n",
    "    data: matrix of n features\n",
    "    cluster_labels: n-lengthed array with i-th value identify cluster for i-th data point\n",
    "    n_samples: number of data points to plot from data\n",
    "    decomposer: used to reduce data dimension to 2 or 3 for plotting, can pass in custom decomposer\n",
    "    dimension: specify whether to display 2D or 3D graph\n",
    "    \"\"\"\n",
    "    if dimension not in [2, 3]:\n",
    "        raise ValueError(\n",
    "            \"Parameter 'dimension' can only have value 2 or 3\"\n",
    "        )\n",
    "    \n",
    "    n_clusters = max(cluster_labels)\n",
    "    samples = np.random.choice(\n",
    "        range(data.shape[0]), size=n_samples, replace=False\n",
    "    )\n",
    "\n",
    "    # Convert data to the appropriate type and sample size\n",
    "    if type(data) != np.ndarray:\n",
    "        data = np.asarray(data[samples, :].todense())\n",
    "    else:\n",
    "        data = data[samples, :]\n",
    "\n",
    "    if decomposer == 'pca':\n",
    "        transformed = PCA(n_components=dimension).fit_transform(data)\n",
    "    elif decomposer == 'tsne':\n",
    "        transformed = TSNE(n_components=dimension).fit_transform(data)\n",
    "    else:\n",
    "        transformed = decomposer.fit_transform(data)\n",
    "    \n",
    "    cluster_colours = [\n",
    "        cm.hsv(i / n_clusters) for i in cluster_labels[samples][:]\n",
    "    ]\n",
    "\n",
    "    if dimension == 2:\n",
    "        ax = plt.figure().add_subplot(projection=None)\n",
    "        ax.scatter(transformed[:, 0], transformed[:, 1], c=cluster_colours)\n",
    "    else: \n",
    "        ax = plt.figure().add_subplot(projection='3d')\n",
    "        ax.scatter(\n",
    "            transformed[:, 0], transformed[:, 1], transformed[:, 2], \n",
    "            c=cluster_colours\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"{decomposer.upper()} Clusters {dimension}D\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "pca_matrix = PCA(n_components=7).fit_transform(\n",
    "    np.asarray(tfidf_matrix1.todense())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "find_optimal_k_cluster_size(pca_matrix, rand_state=357)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For a dataset of $n$ samples, `clusters` is a $n$-lengthed array where the $i$-th number in the array identify the $i$-th sample of which cluster it belongs to\n",
    "    - e.g. `clusters = [3, 2, 2, 0]`, the 0-th data point belongs to cluster 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = SpectralClustering(n_clusters = 8)\n",
    "clusters = kmeans.fit_predict(pca_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(pca_matrix, clusters, dimension=3, n_samples=4000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Find a way to plot the top $n$ words PER cluster - more frequent words\n",
    "- NEED TO MAKE CLUSTERS USEFUL\n",
    "- how to find frequency of word in each clusters?\n",
    "- can we add OR multiply all dimensions for each feature/term to get some aggregate score, and then sort them by their cluster number? e.g. max(cluster['cluster_4'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Coherence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `gensim` - `Coherence Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "lda_u_mass = CoherenceModel(\n",
    "    model=lda_gensim, corpus=bow_matrix2, dictionary=vocab, coherence='u_mass'\n",
    ")\n",
    "\n",
    "lda_c_v = CoherenceModel(\n",
    "    model=lda_gensim, texts=corpus, dictionary=vocab, coherence='c_v'\n",
    ")\n",
    "\n",
    "print(lda_u_mass.get_coherence())\n",
    "print(lda_c_v.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_topic_coherence = sum([t[1] for t in lda_gensim_topics]) / n_topics\n",
    "print(f'Average topic coherence: {avg_topic_coherence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "gensim_model = tfidf_vect\n",
    "term_matrix = bow_matrix1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualsing `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "\n",
    "count_vect.get_feature_names = count_vect.get_feature_names_out\n",
    "pyLDAvis.sklearn.prepare(lda_sklearn, term_matrix, count_vect)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.gensim_models.prepare(gensim_model, term_matrix, vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Term Matrices\n",
    "- **TODO** - reuse cluster plotting function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcrl2000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "061547610e8fceca887a10faa2400f70de22abf81f0fbff289301edcce13ef23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
